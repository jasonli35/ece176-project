{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Size: 600\n",
      "Validation Set Size: 100\n",
      "Test Set Size: 100\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "\n",
    "class CityscapesDataset(Dataset):\n",
    "    def __init__(self, image_list, mask_list, transforms=None):\n",
    "        self.image_list = image_list\n",
    "        self.mask_list = mask_list\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = read_image(self.image_list[idx]).type(torch.float32)\n",
    "        mask = read_image(self.mask_list[idx]).type(torch.long)\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "            # Apply same transforms to mask if necessary, or write custom transform for mask\n",
    "        return image, mask\n",
    "\n",
    "def get_cityscapes_lists(root_dir, split=\"train\"):\n",
    "    images_dir = os.path.join(root_dir, \"leftImg8bit_trainvaltest\", split)\n",
    "    masks_dir = os.path.join(root_dir, \"gtFine_trainvaltest\", split)\n",
    "    images_list = sorted([os.path.join(images_dir, img) for img in os.listdir(images_dir) if img.endswith('_leftImg8bit.png')])\n",
    "    masks_list = []\n",
    "    for city in os.listdir(masks_dir):\n",
    "        city_path = os.path.join(masks_dir, city)  # Path to the city directory\n",
    "        if os.path.isdir(city_path):\n",
    "            masks_list += [os.path.join(city_path, mask) for mask in os.listdir(city_path) if mask.endswith('_labelIds.png')]\n",
    "    sorted(masks_list)\n",
    "    return images_list, masks_list\n",
    "\n",
    "# Define transforms\n",
    "transforms = Compose([\n",
    "#     ToTensor(),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Get data lists\n",
    "root_dir = \".\"\n",
    "train_images, train_masks = get_cityscapes_lists(root_dir, \"train\")\n",
    "val_images, val_masks = get_cityscapes_lists(root_dir, \"val\")\n",
    "test_images, test_masks = get_cityscapes_lists(root_dir, \"test\")\n",
    "\n",
    "train_images = train_images[:600]\n",
    "train_masks = train_masks[:600]\n",
    "val_images = val_images[:100]\n",
    "val_masks = val_masks[:100]\n",
    "test_images = test_images[:100]\n",
    "test_masks = test_masks[:100]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CityscapesDataset(train_images, train_masks, transforms=transforms)\n",
    "val_dataset = CityscapesDataset(val_images, val_masks, transforms=transforms)\n",
    "test_dataset = CityscapesDataset(test_images, test_masks, transforms=transforms)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Print dataset sizes\n",
    "\n",
    "\n",
    "print(\"Training Set Size:\", len(train_dataset))\n",
    "print(\"Validation Set Size:\", len(val_dataset))\n",
    "print(\"Test Set Size:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: torch.Size([1, 3, 1024, 2048])\n",
      "Labels shape: torch.Size([1, 1, 1024, 2048])\n"
     ]
    }
   ],
   "source": [
    "data_iter = iter(train_loader)\n",
    "batch = next(data_iter)\n",
    "\n",
    "# Now, `batch` contains your data and possibly labels depending on your dataset\n",
    "# If your dataset returns a tuple of data and labels, you can do:\n",
    "data, labels = batch\n",
    "\n",
    "# Print the shape of data and labels\n",
    "print(\"Data shape:\", data.shape)\n",
    "print(\"Labels shape:\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=1, shuffle=True\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=1, shuffle=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=1, shuffle=False\n",
    ")\n",
    "\n",
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "\n",
    "print('using device:', device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/home/jal042/private/176_final_project/FCN/trainer.py:30: RuntimeWarning: invalid value encountered in true_divide\n",
      "  iou = np.diag(hist) / (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Model\n",
      "Avg Acc: 0.001077, Mean IoU: 3.474e-05\n",
      "Epochs: 0\n",
      "Epoch Loss: 1.607, Avg Acc: 0.531, Mean IoU: 0.09532\n",
      "Epochs: 1\n",
      "Epoch Loss: 1.431, Avg Acc: 0.5309, Mean IoU: 0.08606\n",
      "Epochs: 2\n",
      "Epoch Loss: 1.375, Avg Acc: 0.5248, Mean IoU: 0.088\n",
      "Epochs: 3\n",
      "Epoch Loss: 1.351, Avg Acc: 0.5042, Mean IoU: 0.09109\n",
      "Epochs: 4\n",
      "Epoch Loss: 1.328, Avg Acc: 0.5242, Mean IoU: 0.08672\n",
      "Epochs: 5\n",
      "Epoch Loss: 1.306, Avg Acc: 0.5125, Mean IoU: 0.08451\n",
      "Epochs: 6\n",
      "Epoch Loss: 1.304, Avg Acc: 0.5359, Mean IoU: 0.08128\n",
      "Epochs: 7\n",
      "Epoch Loss: 1.284, Avg Acc: 0.488, Mean IoU: 0.08267\n",
      "Epochs: 8\n",
      "Epoch Loss: 1.268, Avg Acc: 0.5144, Mean IoU: 0.08832\n",
      "Epochs: 9\n",
      "Epoch Loss: 1.246, Avg Acc: 0.5256, Mean IoU: 0.08578\n",
      "Epochs: 10\n",
      "Epoch Loss: 1.221, Avg Acc: 0.516, Mean IoU: 0.08569\n",
      "Epochs: 11\n",
      "Epoch Loss: 1.205, Avg Acc: 0.4985, Mean IoU: 0.08615\n",
      "Epochs: 12\n",
      "Epoch Loss: 1.174, Avg Acc: 0.5115, Mean IoU: 0.09277\n",
      "Epochs: 13\n",
      "Epoch Loss: 1.146, Avg Acc: 0.5302, Mean IoU: 0.08604\n",
      "Epochs: 14\n",
      "Epoch Loss: 1.114, Avg Acc: 0.4794, Mean IoU: 0.08525\n",
      "Epochs: 15\n",
      "Epoch Loss: 1.079, Avg Acc: 0.5009, Mean IoU: 0.08667\n",
      "Epochs: 16\n",
      "Epoch Loss: 1.033, Avg Acc: 0.5024, Mean IoU: 0.08816\n",
      "Epochs: 17\n",
      "Epoch Loss: 0.9737, Avg Acc: 0.5073, Mean IoU: 0.08849\n",
      "Epochs: 18\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from FCN.trainer import Trainer\n",
    "import torchvision\n",
    "import numpy as np\n",
    "\n",
    "class FCN8s(nn.Module):\n",
    "\n",
    "    def __init__(self, n_class=35):\n",
    "        super(FCN8s, self).__init__()\n",
    "\n",
    "        ################################################################################\n",
    "        # TODO: Implement the layers for FCN8s.                                        #\n",
    "        ################################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size = 3, padding = 100)\n",
    "        self.relu1_1 = nn.ReLU()\n",
    "        \n",
    "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size = 3, padding = 1)\n",
    "        self.relu1_2 = nn.ReLU()\n",
    "        \n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)\n",
    "        \n",
    "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size = 3, padding = 1)\n",
    "        self.relu2_1 = nn.ReLU()\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size = 3, padding = 1)\n",
    "        self.relu2_2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)\n",
    "        \n",
    "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size = 3, padding = 1)\n",
    "        self.relu3_1 = nn.ReLU(inplace=True)\n",
    "        self.conv3_2 =  nn.Conv2d(256, 256, kernel_size = 3, padding = 1)\n",
    "        self.relu3_2 = nn.ReLU()\n",
    "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size = 3, padding = 1)\n",
    "        self.relu3_3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)\n",
    "        \n",
    "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size = 3, padding = 1)\n",
    "        self.relu4_1 = nn.ReLU()\n",
    "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size = 3, padding = 1)\n",
    "        self.relu4_2 = nn.ReLU()\n",
    "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size = 3, padding = 1)\n",
    "        self.relu4_3 = nn.ReLU()\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)\n",
    "        \n",
    "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size = 3, padding = 1)\n",
    "        self.relu5_1 = nn.ReLU()\n",
    "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size = 3, padding = 1)\n",
    "        self.relu5_2 = nn.ReLU()\n",
    "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size = 3, padding = 1)\n",
    "        self.relu5_3 = nn.ReLU()\n",
    "        self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)\n",
    "        \n",
    "        self.fc6 = nn.Conv2d(512, 4096, padding = 0, kernel_size = 7)\n",
    "        self.relufc6 = nn.ReLU()\n",
    "        self.fc7 = nn.Conv2d(4096, 4096, padding = 0, kernel_size = 1)\n",
    "        self.relufc7 = nn.ReLU()\n",
    "        \n",
    "        self.dropout6 = nn.Dropout2d()\n",
    "        self.dropout7 = nn.Dropout2d()\n",
    "        \n",
    "        self.score_lay = nn.Conv2d(4096, n_class, kernel_size = 1, padding = 0)\n",
    "        \n",
    "        self.trans_conv = nn.ConvTranspose2d(n_class, n_class, kernel_size = 4, stride = 2, bias = False)\n",
    "        \n",
    "        self.score_pool_3 = nn.Conv2d(256, n_class, kernel_size = 1)\n",
    "        self.score_pool_4 = nn.Conv2d(512, n_class, kernel_size = 1)\n",
    "        \n",
    "        self.trans_conv_3 = nn.ConvTranspose2d(n_class, n_class, kernel_size = 16, stride = 8, bias = False)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             #\n",
    "        ################################################################################\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def get_upsampling_weight(self, in_channels, out_channels, kernel_size):\n",
    "        \"\"\"Make a 2D bilinear kernel suitable for upsampling\"\"\"\n",
    "        factor = (kernel_size + 1) // 2\n",
    "        if kernel_size % 2 == 1:\n",
    "            center = factor - 1\n",
    "        else:\n",
    "            center = factor - 0.5\n",
    "        og = np.ogrid[:kernel_size, :kernel_size]\n",
    "        filt = (1 - abs(og[0] - center) / factor) * \\\n",
    "               (1 - abs(og[1] - center) / factor)\n",
    "        weight = np.zeros((in_channels, out_channels, kernel_size, kernel_size),\n",
    "                          dtype=np.float64)\n",
    "        weight[range(in_channels), range(out_channels), :, :] = filt\n",
    "        return torch.from_numpy(weight).float()\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                m.weight.data.zero_()\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            if isinstance(m, nn.ConvTranspose2d):\n",
    "                assert m.kernel_size[0] == m.kernel_size[1]\n",
    "                initial_weight = self.get_upsampling_weight(\n",
    "                    m.in_channels, m.out_channels, m.kernel_size[0])\n",
    "                m.weight.data.copy_(initial_weight)\n",
    "\n",
    "                \n",
    "    def forward(self, x):\n",
    "        ################################################################################\n",
    "        # TODO: Implement the forward pass for FCN8s.                                 #\n",
    "        ################################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        input_height = len(x[0][0])\n",
    "        input_width = len(x[0][0][0])\n",
    "        score = self.relu1_1(self.conv1_1(x))\n",
    "        score = self.pool1(self.relu1_2(self.conv1_2(score)))\n",
    "        \n",
    "        score = self.relu2_1(self.conv2_1(score))\n",
    "        score = self.pool2(self.relu2_2(self.conv2_2(score)))\n",
    "        \n",
    "        score = self.relu3_1(self.conv3_1(score))\n",
    "        score = self.relu3_2(self.conv3_2(score))\n",
    "        \n",
    "        score3 = self.pool3(self.relu3_3(self.conv3_3(score)))\n",
    "    \n",
    "        \n",
    "        score = self.relu4_1(self.conv4_1(score3))\n",
    "        score = self.relu4_2(self.conv4_2(score))\n",
    "        score4 = self.pool4(self.relu4_3(self.conv4_3(score)))\n",
    "        \n",
    "        \n",
    "        score = self.relu5_1(self.conv5_1(score4))\n",
    "        score = self.relu5_2(self.conv5_2(score))\n",
    "        score = self.pool5(self.relu5_3(self.conv5_3(score)))\n",
    "        \n",
    "        score = self.dropout6(self.relufc6(self.fc6(score)))\n",
    "        score = self.dropout7(self.relufc7(self.fc7(score)))\n",
    "        score = self.score_lay(score)\n",
    "        \n",
    "        \n",
    "        upscore1 = self.trans_conv(score)\n",
    "        \n",
    "        score_pool4 = self.score_pool_4(score4)[:,:,5:5+len(upscore1[0][0]),5:5+len(upscore1[0][0][0])]\n",
    "        upscore2_input = upscore1 + score_pool4\n",
    "        \n",
    "        upscore2 = self.trans_conv(upscore2_input)\n",
    "        score_pool3 = self.score_pool_3(score3)[:,:,9:9+len(upscore2[0][0]),9:9+len(upscore2[0][0][0])]\n",
    "        \n",
    "        upscore3 = self.trans_conv_3(score_pool3 + upscore2)\n",
    "        upscore3 = upscore3[:,:,31: 31 + input_height, 31:31 + input_width]\n",
    "\n",
    "        \n",
    "        return upscore3\n",
    "        \n",
    "\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             #\n",
    "        ################################################################################\n",
    "\n",
    "        return h\n",
    "\n",
    "    def copy_params_from_vgg16(self, vgg16):\n",
    "        features = [\n",
    "            self.conv1_1, self.relu1_1,\n",
    "            self.conv1_2, self.relu1_2,\n",
    "            self.pool1,\n",
    "            self.conv2_1, self.relu2_1,\n",
    "            self.conv2_2, self.relu2_2,\n",
    "            self.pool2,\n",
    "            self.conv3_1, self.relu3_1,\n",
    "            self.conv3_2, self.relu3_2,\n",
    "            self.conv3_3, self.relu3_3,\n",
    "            self.pool3,\n",
    "            self.conv4_1, self.relu4_1,\n",
    "            self.conv4_2, self.relu4_2,\n",
    "            self.conv4_3, self.relu4_3,\n",
    "            self.pool4,\n",
    "            self.conv5_1, self.relu5_1,\n",
    "            self.conv5_2, self.relu5_2,\n",
    "            self.conv5_3, self.relu5_3,\n",
    "            self.pool5,\n",
    "        ]\n",
    "        for l1, l2 in zip(vgg16.features, features):\n",
    "            if isinstance(l1, nn.Conv2d) and isinstance(l2, nn.Conv2d):\n",
    "                assert l1.weight.size() == l2.weight.size()\n",
    "                assert l1.bias.size() == l2.bias.size()\n",
    "                l2.weight.data.copy_(l1.weight.data)\n",
    "                l2.bias.data.copy_(l1.bias.data)\n",
    "        for i, name in zip([0, 3], ['fc6', 'fc7']):\n",
    "            l1 = vgg16.classifier[i]\n",
    "            l2 = getattr(self, name)\n",
    "            l2.weight.data.copy_(l1.weight.data.view(l2.weight.size()))\n",
    "            l2.bias.data.copy_(l1.bias.data.view(l2.bias.size()))\n",
    "\n",
    "\n",
    "\n",
    "vgg16 = torchvision.models.vgg16(pretrained=True)\n",
    "\n",
    "model8 = FCN8s(n_class=35) #may be 35\n",
    "model8.copy_params_from_vgg16(vgg16)\n",
    "model8.to(device)\n",
    "\n",
    "best_model_fcn8s = Trainer(\n",
    "    model8,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    test_loader, \n",
    "    num_epochs=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
